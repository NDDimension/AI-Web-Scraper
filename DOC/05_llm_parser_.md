# Chapter 5: LLM Parser ğŸ¤–ğŸ”

Welcome back! So far, youâ€™ve learned how to:

- ğŸ–±ï¸ Use the UI and follow the app workflow

- ğŸŒ Fetch raw HTML with the `Web Scraper`

- ğŸ§¹ Clean it up with the `HTML Processor`

- âœ‚ï¸ Break it into smaller pieces using `Content Chunking`

Now it's time for the smart part: LLM Parsing â€” where AI reads the chunks and pulls out exactly what you asked for.

## What Problem Does It Solve? ğŸ§ 

Say you want the price or the author from a long article. Keyword search? Meh. Youâ€™ll miss context or get the wrong match.

The **LLM Parser** understands your request and the content â€” like a clever detective ğŸ•µï¸ â€” and extracts just the right data, no matter how it's worded.

## Key Concepts ğŸ“˜

The LLM Parser relies on these key ideas:

1. **LLMs (Large Language Models):** AI models trained on tons of text (we use Ollama).

2. **Natural Language Understanding:** You describe what you want, like â€œGet the product priceâ€, and the AI gets it.

3. **Prompting:** We give the AI a well-crafted instruction to guide its output.

4. **Chunk-wise Processing:** It reads each content chunk separately.

5. **Aggregation:** The app gathers results from each chunk to form one neat answer.

6. **LangChain:** A library that makes talking to LLMs easy in Python.

By combining these elements, the LLM Parser intelligently finds the precise information you need from the web content.

## How to Use It (UI View) ğŸ§‘â€ğŸ’»

No extra steps. Just:

- Scrape & clean content as usual

- Type your request (e.g. â€œExtract the price.â€)

- Click â€œParse Contentâ€

- Thatâ€™s it! The AI goes to work and shows the result âœ…

## What Happens Internally ğŸ› ï¸

```mermaid
sequenceDiagram
    participant User
    participant UI (main.py)
    participant LLM Parser (parse.py)
    participant Ollama Model

    User->>UI (main.py): Clicks "Parse Content" (with description)
    UI (main.py)->>UI (main.py): Retrieve text chunks (from session_state)
    UI (main.py)->>LLM Parser (parse.py): Send chunks & description (call parse_with_ollama)
    LLM Parser (parse.py)->>LLM Parser (parse.py): Prepare prompt instructions
    LLM Parser (parse.py)->>LLM Parser (parse.py): Loop through each text chunk
    loop For each chunk
        LLM Parser (parse.py)->>Ollama Model: Send prompt + chunk + description
        Ollama Model-->>LLM Parser (parse.py): Return extracted data for this chunk
        LLM Parser (parse.py)->>LLM Parser (parse.py): Store extracted data
    end
    LLM Parser (parse.py)->>LLM Parser (parse.py): Combine stored data
    LLM Parser (parse.py)-->>UI (main.py): Return combined extracted data
    UI (main.py)->>User: Display final result
```

## Core Code Walkthrough ğŸ§©

### **1. Prompt Template ğŸ’¬**

```python
template = (
    "You are tasked with extracting specific information from: {dom_content} \n"
    "Instructions:\n"
    "1. Extract info based on: {parse_description}\n"
    "2. No extra text\n"
    "3. Return empty if nothing found\n"
    "4. Only return the exact data"
)
prompt = ChatPromptTemplate.from_template(template)
```
This tells the AI *what to do, what to look for, and how to respond*(strictly with data!).

### **2. Setting Up Ollama + LangChain ğŸ”Œ**

```python
model = OllamaLLM(model="llama3.2:1b")
chain = prompt | model
```
This combo formats the prompt â• sends it to the AI model.

### **3. Parsing Each Chunk ğŸ”**

```python
def parse_with_ollama(dom_chunks, parse_description):
    parsed_results = []
    for chunk in dom_chunks:
        response = chain.invoke({
            "dom_content": chunk,
            "parse_description": parse_description
        })
        parsed_results.append(response)
    return "\n".join(parsed_results)
```
- Loops through all text chunks

- Sends each to the model with your request

- Collects and combines the results âœ¨

### **4. In main.py ğŸ”§**

```python
if st.button("Parse Content"):
    result = parse_with_ollama(dom_chunks, parse_description)
    st.write(result)
```
One button. One AI. One perfect answer. âœ…

## Conclusion ğŸ¯

The LLM Parser is the brain ğŸ§  of your AI Web Scraper.

It uses smart prompting + powerful language models (via Ollama & LangChain) to read through cleaned, chunked content and pull out exactly what you asked for.

With just a short request like â€œExtract the product nameâ€, the app processes everything in the background â€” no code, no confusion.

This wraps up the journey! ğŸ‰ You now understand every piece of the pipeline: scraping â¡ cleaning â¡ chunking â¡ parsing.

Want to add an export or summary feature next? Letâ€™s keep building! ğŸ’¡


## ğŸ™ Thank You!
Thank you for following along this journey â€” from scraping a messy webpage all the way to extracting precise, AI-powered insights. We hope this tutorial not only helped you understand the inner workings of the AI Web Scraper but also sparked ideas for your own projects.

Your curiosity, creativity, and willingness to explore are what bring tools like this to life. Keep building, keep tinkering, and never stop asking â€œWhat else can I do with this?â€

Happy scraping! ğŸš€
